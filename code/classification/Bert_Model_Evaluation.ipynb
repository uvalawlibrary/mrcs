{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook documents the model fit during the first phase of Modelling Racial Caste System Project for the DistilBERT model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages\n",
    "\n",
    "* More packages get imported in or right before the chunks that apply them to reduce conflict issues and allow for seamless implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.33.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion\n",
    "\n",
    "Load Dataset into Pandas Dataframe. The links below contain helpful information on loading datasets for transformer classsification.\n",
    "\n",
    "https://huggingface.co/docs/datasets/tabular_load#pandas-dataframes  \n",
    "https://huggingface.co/docs/datasets/loading  \n",
    "https://huggingface.co/docs/transformers/tasks/sequence_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version of the training dataset had implicit and extrinsic sentences from the UNC dataset. Which explains the weakened accuracy and F1 scores below. However, even these scores were higher than the Random Forest F1 scores. Because this is one of several experiments, the bottom of the script does not have checkpooint images but we include the results in the description and technical document. Please read the Tech Document to see other experiments that we ran to determine the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "\n",
    "#Implicit and extrinsic sentences from the UNC dataset. \n",
    "df = pd.read_csv(\"fullnjc_dataframe9.csv\")\n",
    "df.drop(columns = 'Unnamed: 0', inplace=True)\n",
    "features = df.loc[:,['year','sentence', 'jim_crow','state']].copy()\n",
    "train, test = train_test_split(df, test_size = 0.2, random_state = 210)\n",
    "\n",
    "#Specify the training and test columns\n",
    "train = train.loc[:,['sentence', 'jim_crow']].copy()\n",
    "test = test.loc[:,['sentence', 'jim_crow']].copy()\n",
    "\n",
    "#Rename the columns\n",
    "train = train.set_index('sentence', inplace=False)\n",
    "test = test.set_index('sentence', inplace=False)\n",
    "train = train.rename(columns={\"jim_crow\": \"label\"})\n",
    "test = test.rename(columns={\"jim_crow\": \"label\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_pandas(train, split=\"train\")\n",
    "test_ds = Dataset.from_pandas(test, split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Analysis Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tokenized and prepared the sentence column for input into the DistilBERT model:\n",
    "\n",
    "* The AutoTokenizer from the transformers library, pre-trained for distilbert-base-uncased, was used for tokenization.\n",
    "* Sentences were tokenized into numerical IDs and attention masks, automatically handling tasks like lowercasing and truncation of sequences longer than the model's maximum length of 512 tokens.\n",
    "* The tokenization process was applied to the training and testing datasets using the map function, ensuring that the outputs were formatted correctly for the DistilBERT model.\n",
    "* Dynamic Padding: We used the DataCollatorWithPadding to dynamically pad sequences in each batch to the length of the longest sequence. This ensures efficient batching and compatibility with the TensorFlow framework, without introducing unnecessary computational overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1fb382e1bde4fa08284c39ac018af5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14468 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8556408a849c4932b8e1464a9af021e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3617 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train = train_ds.map(preprocess_function, batched=True)\n",
    "tokenized_test = test_ds.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-19 21:32:25.439435: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-19 21:32:26.093907: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow specific libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Metrics\n",
    "\n",
    "We designated accuracy and F1 as the primary metrics of interest to evaluate model performance.\n",
    "\n",
    "To implement this:\n",
    "\n",
    "#### Metrics Computation:\n",
    "* A compute_metrics function calculates accuracy and F1 scores by comparing model predictions with true labels.\n",
    "* Predictions are derived from model logits using np.argmax to identify the most probable class.\n",
    "#### Label Mapping:\n",
    "* The id2label and label2id dictionaries map numeric labels (used internally by the model) to descriptive labels (\"jim_crow\" and \"non_jim_crow\") for interpretability.\n",
    "#### Optimizer and Learning Rate Scheduler:\n",
    "* An Adam-based optimizer is configured with an initial learning rate (init_lr=2e-5) and a warm-up schedule to stabilize training.\n",
    "* Training setup includes a batch size of 16, 6 epochs, and dynamically calculated total_train_steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    load_accuracy = evaluate.load(\"accuracy\")\n",
    "    load_f1 = evaluate.load(\"f1\")\n",
    "    \n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy =  load_accuracy.compute(predictions=predictions, references= labels)[\"accuracy\"]\n",
    "    f1 = load_f1.compute(predictions=predictions, references= labels)[\"f1\"]\n",
    "    return {\"accuracy\": accuracy, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"non_jim_crow\", 1: \"jim_crow\"}\n",
    "label2id = {\"non_jim_crow\": 0, \"jim_crow\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "\n",
    "batch_size = 16\n",
    "num_epochs = 6\n",
    "batches_per_epoch = len(tokenized_train) // batch_size\n",
    "total_train_steps = int(batches_per_epoch * num_epochs)\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=2e-5,\n",
    "    num_warmup_steps=5,  # Adjust this based on the size of your dataset. We used 5 because the data is relatively smaller than others in the millions.\n",
    "    num_train_steps=total_train_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Testing\n",
    "\n",
    "We prepared the model and datasets for training, monitored its performance using metrics, and saved the final model for deployment:\n",
    "\n",
    "#### Model Initialization:\n",
    "* The DistilBERT model is loaded with pre-trained weights for text classification.\n",
    "* The model is configured for binary classification with num_labels=2 and uses the label mappings (id2label and label2id) for consistency.\n",
    "#### Dataset Preparation:\n",
    "* Training and validation datasets were prepared using the prepare_tf_dataset method. This ensured proper batching and padding of tokenized data for compatibility with TensorFlow.\n",
    "#### Compilation and Training:\n",
    "* The model was compiled with the Adam-based optimizer.\n",
    "* Training was performed for 6 epochs with a batch size of 16, and validation metrics were monitored using the KerasMetricCallback for accuracy and F1. Epoch 3 had the highest accuracy and F1 scores.\n",
    "#### Saving the Model:\n",
    "* The trained model and tokenizer were saved to \\\"my_bertmodel7\\\" and \\\"tokenizer7\\\", enabling reuse for future predictions or further fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-19 21:32:42.579126: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-19 21:32:45.002744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 77576 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:0f:00.0, compute capability: 8.0\n",
      "2023-11-19 21:32:45.006547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 78300 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:bd:00.0, compute capability: 8.0\n",
      "2023-11-19 21:32:47.705785: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    tokenized_train,\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_validation_set = model.prepare_tf_dataset(\n",
    "    tokenized_test,\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "\n",
    "metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "904/904 [==============================] - 65s 63ms/step - loss: 0.1698 - val_loss: 0.1358 - accuracy: 0.9533 - f1: 0.8776\n",
      "Epoch 2/6\n",
      "904/904 [==============================] - 52s 58ms/step - loss: 0.0947 - val_loss: 0.1179 - accuracy: 0.9569 - f1: 0.8868\n",
      "Epoch 3/6\n",
      "904/904 [==============================] - 52s 58ms/step - loss: 0.0590 - val_loss: 0.1154 - accuracy: 0.9652 - f1: 0.9051\n",
      "Epoch 4/6\n",
      "904/904 [==============================] - 52s 58ms/step - loss: 0.0347 - val_loss: 0.1936 - accuracy: 0.9312 - f1: 0.8365\n",
      "Epoch 5/6\n",
      "904/904 [==============================] - 52s 57ms/step - loss: 0.0215 - val_loss: 0.1394 - accuracy: 0.9627 - f1: 0.8999\n",
      "Epoch 6/6\n",
      "904/904 [==============================] - 52s 58ms/step - loss: 0.0130 - val_loss: 0.1536 - accuracy: 0.9641 - f1: 0.9040\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f54dc1899a0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Chunk\n",
    "\n",
    "callbacks = [metric_callback]\n",
    "model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=6, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"my_bertmodel7\")\n",
    "tokenizer.save_pretrained(\"tokenizer7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the saved my_bertmodel7 to predict on the 10,000 random sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('oct1_10k_samplefixed.csv')\n",
    "df = df.loc[:,['sentence', 'jim_crow']].copy()\n",
    "df = df.rename(columns={\"jim_crow\": \"label\"})\n",
    "\n",
    "# Initialize the UVA model and tokenizer inference pipeline for text classification\n",
    "nlp = pipeline(\"sentiment-analysis\", model=\"my_bertmodel7\", tokenizer=\"tokenizer7\", truncation=True)\n",
    "\n",
    "# Define the name of the text column in your CSV\n",
    "text_column = 'sentence'  \n",
    "\n",
    "# Perform inference and add the inferred labels to a new column\n",
    "df['inferred_label'] = df[text_column].apply(lambda x: nlp(x)[0]['label'])\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df.to_csv('bert10k_with_inference7.csv', index=False)\n",
    "\n",
    "print(\"Inference results saved to 'bert10k_with_inference.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['jim_crow'] = df['inferred_label'].apply(lambda x: 1 if x == 'jim_crow' else 0)\n",
    "df.to_csv('bert10k_with_inference7.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version predicted 9621 non Jim Crow and 379 Jim Crow sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.value_counts([\"jim_crow\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df.loc[df.jim_crow != 0, [\"sentence\",\"jim_crow\",\"inferred_label\"]].sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.jim_crow != 1, [\"sentence\",\"jim_crow\",\"inferred_label\"]].sample(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
